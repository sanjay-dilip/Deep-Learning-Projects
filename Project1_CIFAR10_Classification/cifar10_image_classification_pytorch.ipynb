{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project 1: Computer Vision with PyTorch\n",
    "\n",
    "## Student Information\n",
    "\n",
    "- **Name:** Shreya Raghunath\n",
    "- **Student ID:** 2021771444\n",
    "- **Date:** 20/07/2025\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project demonstrates the complete PyTorch workflow for computer vision tasks using the CIFAR-10 dataset. We will build, train, and evaluate multiple neural network architectures to classify images into 10 different categories.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand the PyTorch workflow for computer vision\n",
    "2. Implement different neural network architectures\n",
    "3. Train and evaluate models on real image data\n",
    "4. Compare model performance and understand trade-offs\n",
    "5. Save and load trained models\n",
    "\n",
    "### Dataset: CIFAR-10\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 different classes:\n",
    "- Airplane, Automobile, Bird, Cat, Deer\n",
    "- Dog, Frog, Horse, Ship, Truck\n",
    "\n",
    "Each class has 6,000 images (5,000 for training, 1,000 for testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "We begin by importing all the necessary libraries for data handling, model building, training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation\n",
    "\n",
    "We load the CIFAR-10 dataset using `torchvision.datasets` and apply appropriate transformations:\n",
    "\n",
    "- **ToTensor**: Converts images to tensors\n",
    "- **Normalization**: Scales pixel values to a range that accelerates training\n",
    "\n",
    "**We also define:**\n",
    "- **Training**, **validation**, and **test** splits\n",
    "- **Data loaders** for batch processing\n",
    "\n",
    "**Findings:**\n",
    "-\tToTensor() scaled images to [0, 1] and converted them to PyTorch tensors.\n",
    "-\tNormalization with CIFAR-10-specific mean and std ([0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]) improved model convergence.\n",
    "-\tProper batching helped in speeding up GPU training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "# We'll normalize the images to help with training stability\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.RandomRotation(10),      # Data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Visualization\n",
    "\n",
    "Before training, we visualize a batch of images and their corresponding class labels to confirm successful loading and to get a feel for the image quality and variability.\n",
    "\n",
    "**Findings:**\n",
    "- Dataset loaded correctly.\n",
    "- Classes appear well-distributed.\n",
    "- No image corruption or skew observed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show images\n",
    "def imshow(img):\n",
    "    # Denormalize the image\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    imshow(torchvision.utils.make_grid(images[i]))\n",
    "    plt.title(classes[labels[i]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Image shape: {images[0].shape}\")\n",
    "print(f\"Number of channels: {images[0].shape[0]}\")\n",
    "print(f\"Image height: {images[0].shape[1]}\")\n",
    "print(f\"Image width: {images[0].shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model 1 - Simple Feedforward Neural Network\n",
    "\n",
    "As a baseline, we train a **fully connected feedforward neural network** (FCNN) on flattened image vectors.\n",
    "\n",
    "This approach helps us:\n",
    "- Establish a benchmark before using CNNs\n",
    "- Observe the limitations of FCNNs for image data (e.g., lack of spatial locality awareness)\n",
    "\n",
    "**Findings :**\n",
    "\n",
    "- FCNN lacks ability to capture spatial patterns.\n",
    "- Low accuracy (~40-50%) due to flattened image input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=3072, hidden_size=512, num_classes=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model1 = SimpleNN().to(device)\n",
    "print(\"Model 1 - Simple Feedforward Neural Network:\")\n",
    "print(model1)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model1.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Function\n",
    "\n",
    "Let's create a training function that we can reuse for all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and return training history\n",
    "    \"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if i % 100 == 99:\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(running_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100 * correct / total\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Train Acc: {train_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
    "    \n",
    "    print('Finished Training!')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Model 1\n",
    "\n",
    "Now let's train our first model and see how it performs.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- Training function worked across models.\n",
    "- Overfitting observed due to model limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1\n",
    "print(\"Training Model 1 - Simple Feedforward Neural Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history1 = train_model(model1, trainloader, testloader, epochs=10, learning_rate=0.001)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history1['train_accuracies'], label='Training Accuracy')\n",
    "plt.plot(history1['test_accuracies'], label='Test Accuracy')\n",
    "plt.title('Model 1 - Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history1['train_losses'], label='Training Loss')\n",
    "plt.title('Model 1 - Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {history1['test_accuracies'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model 2 - Convolutional Neural Network (CNN)\n",
    "\n",
    "We now implement a CNN that leverages spatial information in images using convolutional and pooling layers. CNNs are better suited for image classification tasks.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- CNN outperformed FCNN (accuracy ~70-80%).\n",
    "- Use of Conv2D, MaxPooling2D, Dropout, and Flatten enabled deep feature learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Third conv block\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the CNN model\n",
    "model2 = CNN().to(device)\n",
    "print(\"Model 2 - Convolutional Neural Network:\")\n",
    "print(model2)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model2.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train Model 2\n",
    "\n",
    "Let's train our CNN model and compare its performance with the simple feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 2\n",
    "print(\"Training Model 2 - Convolutional Neural Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history2 = train_model(model2, trainloader, testloader, epochs=15, learning_rate=0.001)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history2['train_accuracies'], label='Training Accuracy')\n",
    "plt.plot(history2['test_accuracies'], label='Test Accuracy')\n",
    "plt.title('Model 2 - CNN Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history2['train_losses'], label='Training Loss')\n",
    "plt.title('Model 2 - CNN Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {history2['test_accuracies'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Comparison\n",
    "\n",
    "Let's compare the performance of both models to understand the differences.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- Plotting test accuracy over epochs for Simple NN and CNN → CNN consistently achieves higher test accuracy, ending at 80.73% vs 45.50% for Simple NN.\n",
    "\n",
    "- Plotting training accuracy over epochs for both models → CNN shows stronger learning with steadily increasing training accuracy.\n",
    "\n",
    "- CNN clearly outperforms Simple NN in both metrics.\n",
    "\n",
    "- CNN shows a 35.23 percentage point improvement over Simple NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot test accuracies\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history1['test_accuracies'], label='Simple NN', marker='o')\n",
    "plt.plot(history2['test_accuracies'], label='CNN', marker='s')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracies\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history1['train_accuracies'], label='Simple NN', marker='o')\n",
    "plt.plot(history2['train_accuracies'], label='CNN', marker='s')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Bar plot of final accuracies\n",
    "plt.subplot(1, 3, 3)\n",
    "models = ['Simple NN', 'CNN']\n",
    "final_test_acc = [history1['test_accuracies'][-1], history2['test_accuracies'][-1]]\n",
    "final_train_acc = [history1['train_accuracies'][-1], history2['train_accuracies'][-1]]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, final_train_acc, width, label='Training Accuracy', alpha=0.8)\n",
    "plt.bar(x + width/2, final_test_acc, width, label='Test Accuracy', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Final Model Performance')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Simple NN - Final Test Accuracy: {history1['test_accuracies'][-1]:.2f}%\")\n",
    "print(f\"CNN - Final Test Accuracy: {history2['test_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Improvement: {history2['test_accuracies'][-1] - history1['test_accuracies'][-1]:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 10: Confusion Matrix\n",
    "\n",
    "The confusion matrix gives a visual breakdown of correct vs incorrect predictions per class. It helps identify which classes the model confuses most often.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- Evaluates the trained CNN model on the test dataset using evaluate_model() → returns predictions, true labels, and overall accuracy.\n",
    "\n",
    "- Computes and displays the confusion matrix → shows correct vs incorrect predictions for each class. Most classes like automobile, frog, ship, truck have strong diagonals (high correct counts), indicating good performance.\n",
    "\n",
    "- Calculates per-class accuracy from confusion matrix → cat has the lowest at 58.2%, while ship has the highest at 90.5%, revealing class-wise variation in performance.\n",
    "\n",
    "- Visualizes per-class accuracy as a bar chart → quickly highlights which classes the model performs best and worst on.\n",
    "\n",
    "- overall accuracy is 81%, with solid metrics for most classes, though cat and bird show relatively lower f1-scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testloader, classes):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions, true labels, and confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return all_predictions, all_labels, accuracy\n",
    "\n",
    "# Evaluate the CNN model\n",
    "print(\"Evaluating CNN Model...\")\n",
    "predictions, true_labels, accuracy = evaluate_model(model2, testloader, classes)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix - CNN Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(classes, class_accuracy, color='skyblue', alpha=0.7)\n",
    "plt.title('Per-Class Accuracy - CNN Model')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, class_accuracy):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{acc:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Predictions on Sample Images\n",
    "\n",
    "Let's test our model on some sample images to see how it performs visually.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- Selects a few test images from the dataset and passes them through the trained CNN model → generates predictions for visual inspection.\n",
    "\n",
    "- Denormalizes images before displaying → triggers a warning because some pixel values fall outside the valid display range, but plots still render correctly.\n",
    "\n",
    "- Displays the sample images in a 2x4 grid with predicted vs true labels → uses green titles for correct predictions (✓) and red for incorrect ones (✗).\n",
    "\n",
    "- Helps visually assess model performance on real samples → shows that the model makes mostly correct predictions, with occasional misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, testloader, num_samples=8):\n",
    "    \"\"\"\n",
    "    Make predictions on sample images and visualize results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test images\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images[:num_samples].to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        \n",
    "        # Denormalize image\n",
    "        img = images[i] / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "        # Set title with prediction result\n",
    "        true_label = classes[labels[i]]\n",
    "        pred_label = classes[predicted[i]]\n",
    "        \n",
    "        if true_label == pred_label:\n",
    "            color = 'green'\n",
    "            title = f'✓ {pred_label}\\n(True: {true_label})'\n",
    "        else:\n",
    "            color = 'red'\n",
    "            title = f'✗ {pred_label}\\n(True: {true_label})'\n",
    "        \n",
    "        plt.title(title, color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test predictions on sample images\n",
    "print(\"Testing CNN Model on Sample Images:\")\n",
    "predict_and_visualize(model2, testloader, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save and Load Model\n",
    "\n",
    "Let's save our best performing model and demonstrate how to load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "print(\"Saving the CNN model...\")\n",
    "torch.save({\n",
    "    'model_state_dict': model2.state_dict(),\n",
    "    'optimizer_state_dict': optim.Adam(model2.parameters()).state_dict(),\n",
    "    'epoch': 15,\n",
    "    'accuracy': history2['test_accuracies'][-1],\n",
    "    'model_class': CNN\n",
    "}, 'cifar10_cnn_model.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Load the model back\n",
    "print(\"\\nLoading the saved model...\")\n",
    "checkpoint = torch.load('cifar10_cnn_model.pth', weights_only=False)  # Add weights_only=False\n",
    "\n",
    "# Create a new model instance\n",
    "loaded_model = CNN().to(device)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Test the loaded model\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a single batch\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Make prediction\n",
    "    outputs = loaded_model(images[:1].to(device))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    print(f\"Sample prediction - Predicted: {classes[predicted[0]]}, True: {classes[labels[0]]}\")\n",
    "    print(f\"Model accuracy from checkpoint: {checkpoint['accuracy']:.2f}%\")\n",
    "\n",
    "print(\"Model loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Model Analysis and Insights\n",
    "\n",
    "Let's analyze our models and draw some insights from the results.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- Compares total trainable parameters → CNN has fewer parameters (1.15M) than Simple NN (1.57M), meaning it's more efficient at learning with fewer weights.\n",
    "\n",
    "- Evaluates model performance → CNN outperforms Simple NN with a 35.23 percentage point higher final accuracy (80.73% vs 45.50%).\n",
    "\n",
    "- Looks at training duration → CNN trains longer (15 epochs) but achieves much better accuracy, showing effective learning over time.\n",
    "\n",
    "- Analyzes overfitting by comparing train and test accuracy → Simple NN slightly underfits (-2.32%), while CNN slightly overfits (4.63%), yet generalizes well overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model complexity comparison\n",
    "model1_params = sum(p.numel() for p in model1.parameters())\n",
    "model2_params = sum(p.numel() for p in model2.parameters())\n",
    "\n",
    "print(\"Model Complexity Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Simple NN Parameters: {model1_params:,}\")\n",
    "print(f\"CNN Parameters: {model2_params:,}\")\n",
    "print(f\"Parameter Ratio (CNN/Simple): {model2_params/model1_params:.2f}x\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Simple NN Final Accuracy: {history1['test_accuracies'][-1]:.2f}%\")\n",
    "print(f\"CNN Final Accuracy: {history2['test_accuracies'][-1]:.2f}%\")\n",
    "print(f\"Accuracy Improvement: {history2['test_accuracies'][-1] - history1['test_accuracies'][-1]:.2f} percentage points\")\n",
    "\n",
    "# Training time analysis (approximate)\n",
    "print(f\"\\nTraining Efficiency:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Simple NN Training Epochs: 10\")\n",
    "print(f\"CNN Training Epochs: 15\")\n",
    "print(f\"CNN achieved higher accuracy in more epochs\")\n",
    "\n",
    "# Overfitting analysis\n",
    "simple_nn_overfit = history1['train_accuracies'][-1] - history1['test_accuracies'][-1]\n",
    "cnn_overfit = history2['train_accuracies'][-1] - history2['test_accuracies'][-1]\n",
    "\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Simple NN Overfitting: {simple_nn_overfit:.2f} percentage points\")\n",
    "print(f\"CNN Overfitting: {cnn_overfit:.2f} percentage points\")\n",
    "\n",
    "if cnn_overfit < simple_nn_overfit:\n",
    "    print(\"CNN shows better generalization (less overfitting)\")\n",
    "else:\n",
    "    print(\"Simple NN shows better generalization (less overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Conclusion and Summary\n",
    "\n",
    "In this project, we built and improved deep learning models for CIFAR-10 image classification. We observed how CNNs outperform FCNNs and learned how regularization and data augmentation enhance generalization. This hands-on experience strengthens our understanding of model development, tuning, and evaluation in computer vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"Deep Learning Project 1 - Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nWhat we accomplished:\")\n",
    "print(\"1. ✅ Set up PyTorch environment and loaded CIFAR-10 dataset\")\n",
    "print(\"2. ✅ Built and trained a simple feedforward neural network\")\n",
    "print(\"3. ✅ Built and trained a convolutional neural network (CNN)\")\n",
    "print(\"4. ✅ Compared model performances and analyzed results\")\n",
    "print(\"5. ✅ Evaluated the best model with confusion matrix and per-class accuracy\")\n",
    "print(\"6. ✅ Saved and loaded the trained model\")\n",
    "print(\"7. ✅ Analyzed model complexity and overfitting\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"• CNN achieved {history2['test_accuracies'][-1]:.2f}% accuracy vs {history1['test_accuracies'][-1]:.2f}% for simple NN\")\n",
    "print(f\"• CNN has {model2_params:,} parameters vs {model1_params:,} for simple NN\")\n",
    "print(f\"• CNN shows {'better' if cnn_overfit < simple_nn_overfit else 'worse'} generalization\")\n",
    "\n",
    "print(\"\\nLessons Learned:\")\n",
    "print(\"• Convolutional layers are essential for image classification tasks\")\n",
    "print(\"• Data augmentation helps improve model generalization\")\n",
    "print(\"• Proper model evaluation requires multiple metrics\")\n",
    "print(\"• Model complexity should be balanced with performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "\n",
    "1. **PyTorch Documentation**: https://pytorch.org/docs/\n",
    "2. **CIFAR-10 Dataset**: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "3. **Convolutional Neural Networks**: https://cs231n.github.io/convolutional-networks/\n",
    "4. **Deep Learning Fundamentals**: https://www.deeplearningbook.org/\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
